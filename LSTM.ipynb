{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         25000 non-null  object\n",
      " 1   sentiment  25000 non-null  int64 \n",
      " 2   review     25000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>3453_3</td>\n",
       "      <td>0</td>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "      <td>seems like consideration gone imdb reviews fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>5064_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "      <td>believe made film completely unnecessary first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>10905_3</td>\n",
       "      <td>0</td>\n",
       "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
       "      <td>guy loser get girls needs build picked stronge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>10194_3</td>\n",
       "      <td>0</td>\n",
       "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
       "      <td>minute documentary bu uel made early one spain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>8478_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "      <td>saw movie child broke heart story unfinished e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review  \\\n",
       "0       5814_8          1  With all this stuff going down at the moment w...   \n",
       "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2       7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3       3630_4          0  It must be assumed that those who praised this...   \n",
       "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "...        ...        ...                                                ...   \n",
       "24995   3453_3          0  It seems like more consideration has gone into...   \n",
       "24996   5064_1          0  I don't believe they made this film. Completel...   \n",
       "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...   \n",
       "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...   \n",
       "24999   8478_8          1  I saw this movie as a child and it broke my he...   \n",
       "\n",
       "                                          cleaned_review  \n",
       "0      stuff going moment mj started listening music ...  \n",
       "1      classic war worlds timothy hines entertaining ...  \n",
       "2      film starts manager nicholas bell giving welco...  \n",
       "3      must assumed praised film greatest filmed oper...  \n",
       "4      superbly trashy wondrously unpretentious explo...  \n",
       "...                                                  ...  \n",
       "24995  seems like consideration gone imdb reviews fil...  \n",
       "24996  believe made film completely unnecessary first...  \n",
       "24997  guy loser get girls needs build picked stronge...  \n",
       "24998  minute documentary bu uel made early one spain...  \n",
       "24999  saw movie child broke heart story unfinished e...  \n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "def clean(review):\n",
    "    clean_html = BeautifulSoup(review).get_text()\n",
    "    clean_non_letters = re.sub(\"[^a-zA-Z]\", \" \", clean_html) \n",
    "    cleaned_lowercase = clean_non_letters.lower()\n",
    "    words = cleaned_lowercase.split()\n",
    "    cleaned_words = [w for w in words if w not in stop_words] \n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "df[\"cleaned_review\"] = df[\"review\"].apply(clean)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (17500,)\n",
      "Test set size: (7500,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_review\"], df[\"sentiment\"], test_size = 0.3, \n",
    "                                                    random_state = 10)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(sample, y, tokenizer, max_length = 256):\n",
    "    tokens = tokenizer(sample)[:max_length]\n",
    "    return {'tokens': tokens, 'length': len(tokens), 'sentiment': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [tokenize_data(x, y, tokenizer) for x, y in zip(X_train, y_train)]\n",
    "test_data = [tokenize_data(x, y, tokenizer) for x, y in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62237"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    [d['tokens'] for d in train_data],\n",
    "    specials = ['<unk>', '<pad>']\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_data)):\n",
    "    tokens = train_data[idx][\"tokens\"]\n",
    "    ids = [vocab[token] for token in tokens]\n",
    "    train_data[idx][\"ids\"] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test_data)):\n",
    "    tokens = test_data[idx][\"tokens\"]\n",
    "    ids = [vocab[token] for token in tokens]\n",
    "    test_data[idx][\"ids\"] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,\n",
    "                 dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = 0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,\n",
    "                            dropout=dropout_rate, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, ids, length):\n",
    "        # ids = [batch size, seq len]\n",
    "        # length = [batch size]\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True, \n",
    "                                                            enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # output = [batch size, seq len, hidden dim * n directions]\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
    "            # hidden = [batch size, hidden dim * 2]\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "            # hidden = [batch size, hidden dim]\n",
    "        prediction = self.fc(hidden)\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(vocab_size = len(vocab), embedding_dim = 300, hidden_dim = 300, output_dim = 2, n_layers = 2,\n",
    "             bidirectional = True, dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(62237, 300, padding_idx=0)\n",
       "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=600, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.FastText()\n",
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
    "model.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    batch_ids = [torch.tensor(i[\"ids\"]) for i in batch]\n",
    "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=0, batch_first=True)\n",
    "    batch_length = [i[\"length\"] for i in batch]\n",
    "    batch_label = torch.tensor([i[\"sentiment\"] for i in batch])\n",
    "    return {\"ids\": batch_ids, \"length\": batch_length, \"label\": batch_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, \n",
    "                                               batch_size = 64, \n",
    "                                               collate_fn = collate, \n",
    "                                               shuffle = True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size = 64, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
    "        ids = batch['ids'].to(device)\n",
    "        length = batch['length']\n",
    "        label = batch['label'].to(device)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return epoch_losses, epoch_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
    "            ids = batch['ids'].to(device)\n",
    "            length = batch['length']\n",
    "            label = batch['label'].to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return epoch_losses, epoch_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.79it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 44.52it/s]\n",
      "epoch: 1\n",
      "train_loss: 0.478, train_acc: 0.765\n",
      "test_loss: 0.385, test_acc: 0.830\n",
      "training...: 100%|██████████| 274/274 [00:24<00:00, 10.96it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 47.81it/s]\n",
      "epoch: 2\n",
      "train_loss: 0.318, train_acc: 0.867\n",
      "test_loss: 0.297, test_acc: 0.880\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.69it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 39.45it/s]\n",
      "epoch: 3\n",
      "train_loss: 0.231, train_acc: 0.911\n",
      "test_loss: 0.282, test_acc: 0.882\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.67it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 42.18it/s]\n",
      "epoch: 4\n",
      "train_loss: 0.170, train_acc: 0.935\n",
      "test_loss: 0.308, test_acc: 0.888\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.67it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 41.01it/s]\n",
      "epoch: 5\n",
      "train_loss: 0.127, train_acc: 0.954\n",
      "test_loss: 0.293, test_acc: 0.888\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.79it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 41.72it/s]\n",
      "epoch: 6\n",
      "train_loss: 0.090, train_acc: 0.968\n",
      "test_loss: 0.414, test_acc: 0.879\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.69it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:03<00:00, 39.02it/s]\n",
      "epoch: 7\n",
      "train_loss: 0.070, train_acc: 0.976\n",
      "test_loss: 0.386, test_acc: 0.891\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.83it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 42.13it/s]\n",
      "epoch: 8\n",
      "train_loss: 0.059, train_acc: 0.979\n",
      "test_loss: 0.376, test_acc: 0.881\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.61it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 45.21it/s]\n",
      "epoch: 9\n",
      "train_loss: 0.048, train_acc: 0.983\n",
      "test_loss: 0.460, test_acc: 0.892\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.83it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:03<00:00, 38.97it/s]\n",
      "epoch: 10\n",
      "train_loss: 0.038, train_acc: 0.988\n",
      "test_loss: 0.526, test_acc: 0.883\n",
      "training...: 100%|██████████| 274/274 [00:24<00:00, 10.96it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 42.21it/s]\n",
      "epoch: 11\n",
      "train_loss: 0.037, train_acc: 0.986\n",
      "test_loss: 0.606, test_acc: 0.878\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.93it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 42.07it/s]\n",
      "epoch: 12\n",
      "train_loss: 0.024, train_acc: 0.991\n",
      "test_loss: 0.523, test_acc: 0.891\n",
      "training...: 100%|██████████| 274/274 [00:24<00:00, 11.05it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 45.15it/s]\n",
      "epoch: 13\n",
      "train_loss: 0.030, train_acc: 0.990\n",
      "test_loss: 0.648, test_acc: 0.884\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.88it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 40.76it/s]\n",
      "epoch: 14\n",
      "train_loss: 0.020, train_acc: 0.993\n",
      "test_loss: 0.568, test_acc: 0.887\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.95it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 46.39it/s]\n",
      "epoch: 15\n",
      "train_loss: 0.014, train_acc: 0.994\n",
      "test_loss: 0.646, test_acc: 0.887\n",
      "training...: 100%|██████████| 274/274 [00:24<00:00, 11.04it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 42.27it/s]\n",
      "epoch: 16\n",
      "train_loss: 0.013, train_acc: 0.995\n",
      "test_loss: 0.741, test_acc: 0.883\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.58it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 44.86it/s]\n",
      "epoch: 17\n",
      "train_loss: 0.023, train_acc: 0.992\n",
      "test_loss: 0.667, test_acc: 0.879\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.83it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 40.23it/s]\n",
      "epoch: 18\n",
      "train_loss: 0.012, train_acc: 0.996\n",
      "test_loss: 0.749, test_acc: 0.883\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.69it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 41.30it/s]\n",
      "epoch: 19\n",
      "train_loss: 0.011, train_acc: 0.995\n",
      "test_loss: 0.859, test_acc: 0.882\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.68it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 42.08it/s]\n",
      "epoch: 20\n",
      "train_loss: 0.009, train_acc: 0.997\n",
      "test_loss: 0.625, test_acc: 0.883\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.65it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 41.94it/s]\n",
      "epoch: 21\n",
      "train_loss: 0.008, train_acc: 0.997\n",
      "test_loss: 0.629, test_acc: 0.886\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.59it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:03<00:00, 37.81it/s]\n",
      "epoch: 22\n",
      "train_loss: 0.007, train_acc: 0.998\n",
      "test_loss: 0.629, test_acc: 0.872\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.80it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 42.33it/s]\n",
      "epoch: 23\n",
      "train_loss: 0.008, train_acc: 0.997\n",
      "test_loss: 0.820, test_acc: 0.882\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.65it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 45.74it/s]\n",
      "epoch: 24\n",
      "train_loss: 0.009, train_acc: 0.996\n",
      "test_loss: 0.843, test_acc: 0.866\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.68it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 40.21it/s]\n",
      "epoch: 25\n",
      "train_loss: 0.006, train_acc: 0.997\n",
      "test_loss: 0.742, test_acc: 0.882\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.65it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 40.82it/s]\n",
      "epoch: 26\n",
      "train_loss: 0.007, train_acc: 0.998\n",
      "test_loss: 0.804, test_acc: 0.883\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.71it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:03<00:00, 38.51it/s]\n",
      "epoch: 27\n",
      "train_loss: 0.007, train_acc: 0.997\n",
      "test_loss: 0.706, test_acc: 0.882\n",
      "training...: 100%|██████████| 274/274 [00:24<00:00, 11.21it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 41.18it/s]\n",
      "epoch: 28\n",
      "train_loss: 0.005, train_acc: 0.998\n",
      "test_loss: 0.754, test_acc: 0.887\n",
      "training...: 100%|██████████| 274/274 [00:25<00:00, 10.96it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 52.03it/s]\n",
      "epoch: 29\n",
      "train_loss: 0.004, train_acc: 0.998\n",
      "test_loss: 0.698, test_acc: 0.884\n",
      "training...: 100%|██████████| 274/274 [00:24<00:00, 11.15it/s]\n",
      "evaluating...: 100%|██████████| 118/118 [00:02<00:00, 40.59it/s]\n",
      "epoch: 30\n",
      "train_loss: 0.007, train_acc: 0.997\n",
      "test_loss: 0.679, test_acc: 0.883\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accs.extend(train_acc)\n",
    "    test_losses.extend(test_loss)\n",
    "    test_accs.extend(test_acc)\n",
    "    \n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    epoch_train_acc = np.mean(train_acc)\n",
    "    epoch_test_loss = np.mean(test_loss)\n",
    "    epoch_test_acc = np.mean(test_acc)\n",
    "    \n",
    "    print(f'epoch: {epoch+1}')\n",
    "    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
    "    print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
